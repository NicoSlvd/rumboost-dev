{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rumboost.rumboost import rum_train\n",
    "from rumboost.datasets import load_preprocess_LPMC\n",
    "from rumboost.metrics import cross_entropy\n",
    "\n",
    "import lightgbm\n",
    "import hyperopt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Cross-nested logit model (correlation amongst alternative)\n",
    "\n",
    "This notebook shows features implemented in RUMBoost through an example on the LPMC dataset, a mode choice dataset in London developed Hillel et al. (2018). You can find the original source of data [here](https://www.icevirtuallibrary.com/doi/suppl/10.1680/jsmic.17.00018) and the original paper [here](https://www.icevirtuallibrary.com/doi/full/10.1680/jsmic.17.00018).\n",
    "\n",
    "We first load the preprocessed dataset and its folds for cross-validation. You can find the data under the Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "LPMC_train, LPMC_test, folds = load_preprocess_LPMC(path=\"../Data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Nested Logit model\n",
    "\n",
    "We relax the assumption that the error term is distributed i.i.d. We assume that alternatives are correlated amongst several nests to obtain a cross-nested logit-like model. Cross-Nested logit probabilities are implemented in RUMBoost. The additional parameters, the scale of a nest $\\mu$ and the membership of alternatives to nests, are treated as hyperparameters.\n",
    "\n",
    "Training a cross-nested logit-like rumboost model requires two additional arguments:\n",
    "\n",
    "- ```alphas```: a 2d numpy array of the form ```np.array([[alpha_00, alpha_01, alpha_02],[alpha_10, alpha_11, alpha_12], [alpha_20, alpha_21, alpha_22], [alpha_30, alpha_31, alpha_32]]``` where ```alpha_ij``` means the degree of membership of alternative ```i``` to nest ```j```\n",
    "- ```mu```: a numpy array containing the values (as float) of mu for each nest, e.g. ```[mu_nest_0, mu_nest_1, mu_nest_2]```\n",
    "\n",
    "We test here a cross-nested logit model where the two nests are motorized and flexible. As this is a work in progress, we just arbitrarily choose values of mu and alphas. This will be later chosen with hyperparameter tuning or through scipy.minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parameters\n",
    "\n",
    "You can find an example of general parameters below. Unless stated otherwise, the parameters are the same than in LightGBM, since these parameters are applied directly to LightGBM Booster objects. You can find more information in the LightGBM [docs](https://lightgbm.readthedocs.io/en/stable/Parameters.html#).  For a simple RUMBoost, we recommend letting most of the parameters with default values, as RUMBoost is less sensitive to overfitting. **For a multiclass classification problem, you need to specify the num_classes parameter with the appropriate number of classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "general_params = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_classes\": 4,  # important\n",
    "    \"verbosity\": 1,  # specific RUMBoost parameter\n",
    "    \"num_iterations\": 3000,\n",
    "    \"early_stopping_round\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Utility Model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rum_structure = [\n",
    "    {\n",
    "        \"utility\": [0],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_walking\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "    {\n",
    "        \"utility\": [1],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_cycling\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "    {\n",
    "        \"utility\": [2],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_pt_access\",\n",
    "            \"dur_pt_bus\",\n",
    "            \"dur_pt_rail\",\n",
    "            \"dur_pt_int_waiting\",\n",
    "            \"dur_pt_int_walking\",\n",
    "            \"pt_n_interchanges\",\n",
    "            \"cost_transit\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "                [17],\n",
    "                [18],\n",
    "                [19],\n",
    "                [20],\n",
    "                [21],\n",
    "                [22],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "    {\n",
    "        \"utility\": [3],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_driving\",\n",
    "            \"cost_driving_fuel\",\n",
    "            \"congestion_charge\",\n",
    "            \"driving_traffic_percent\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "                [17],\n",
    "                [18],\n",
    "                [19],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mu$ and $\\alpha$ hyperparameter search\n",
    "\n",
    "We treat $\\mu$ as a hyperparameter. We use hyperopt to find the optimal value of the hyperparameter. More details on how to use hyperopt [here](https://hyperopt.github.io/hyperopt/).\n",
    "\n",
    "Note that for computational purposes, we show here a hyperparameter search for one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([1.25, 1.16]) #random values\n",
    "\n",
    "alphas  = np.array([[0., 1.],\n",
    "                    [0., 1.],\n",
    "                    [1., 0.],\n",
    "                    [0.5, 0.5]])\n",
    "\n",
    "cross_nested_structure = {\n",
    "    \"mu\":mu,\n",
    "    \"alphas\":alphas,\n",
    "    \"optimise_mu\":False,\n",
    "    \"optimise_alphas\":False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specification = {\n",
    "    \"rum_structure\": rum_structure,\n",
    "    \"cross_nested_logit\": cross_nested_structure,\n",
    "    \"general_params\": general_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features and label column names\n",
    "features = [f for f in LPMC_train.columns if f != \"choice\"]\n",
    "label = \"choice\"\n",
    "\n",
    "#create lightgbm dataset\n",
    "lgb_train_set = lightgbm.Dataset(LPMC_train[features], label=LPMC_train[label], free_raw_data=False)\n",
    "lgb_test_set = lightgbm.Dataset(LPMC_test[features], label=LPMC_test[label], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifiy seach of mu\n",
    "param_space = {\n",
    "    \"mu_0\": hyperopt.hp.uniform(\"mu_0\", 1, 2),\n",
    "    \"mu_1\": hyperopt.hp.uniform(\"mu_1\", 1, 2),\n",
    "    \"alpha_14\": hyperopt.hp.uniform(\"alpha_14\", 0, 1),\n",
    "}\n",
    "\n",
    "\n",
    "# objective for hyperopt\n",
    "def objective(space):\n",
    "\n",
    "    # create mu structure\n",
    "    cross_nested_structure[\"mu\"] = np.array([space[\"mu_0\"], space[\"mu_1\"]])\n",
    "    cross_nested_structure[\"alphas\"] = np.array(\n",
    "        [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [space[\"alpha_14\"], 1 - space[\"alpha_14\"]]]\n",
    "    )\n",
    "\n",
    "    ce_loss = 0\n",
    "    num_trees = 0\n",
    "\n",
    "    for train_idx, test_idx in folds:\n",
    "        train_set = lgb_train_set.subset(sorted(train_idx))\n",
    "        test_set = lgb_train_set.subset(sorted(test_idx))\n",
    "\n",
    "        LPMC_model_trained = rum_train(\n",
    "            train_set, model_specification, valid_sets=[test_set]\n",
    "        )\n",
    "\n",
    "        ce_loss += LPMC_model_trained.best_score\n",
    "        num_trees += LPMC_model_trained.best_iteration\n",
    "\n",
    "    ce_loss = ce_loss / 5\n",
    "    num_trees = num_trees / 5\n",
    "\n",
    "    return {\"loss\": ce_loss, \"status\": hyperopt.STATUS_OK, \"best_iteration\": num_trees}\n",
    "\n",
    "\n",
    "# %%\n",
    "# n_iter=25\n",
    "n_iter = 1\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "best_classifier = hyperopt.fmin(\n",
    "    fn=objective,\n",
    "    space=param_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=n_iter,\n",
    "    trials=trials,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'Best mu_0: {best_classifier[\"mu_0\"]} \\n Best mu_1: {best_classifier[\"mu_1\"]} \\n Best alphas: {best_classifier[\"alpha_14\"]} \\n Best negative CE: {trials.best_trial[\"result\"][\"loss\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "We use the value of a previous hyperparameter search and search for the optimal number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, folds = load_preprocess_LPMC(path=\"../Data/\")\n",
    "\n",
    "mu = np.array([1.81, 1.]) #random values\n",
    "\n",
    "alphas  = np.array([[0., 1.],\n",
    "                    [0., 1.],\n",
    "                    [1., 0.],\n",
    "                    [0.364, 0.636]])\n",
    "\n",
    "cross_nested_structure = {\n",
    "    \"mu\":mu,\n",
    "    \"alphas\":alphas,\n",
    "    \"optimise_mu\":False,\n",
    "    \"optimise_alphas\":False,\n",
    "}\n",
    "\n",
    "model_specification['cross_nested_logit'] = cross_nested_structure\n",
    "\n",
    "ce_loss = 0\n",
    "num_trees = 0\n",
    "\n",
    "#5-fold CV\n",
    "for i, (train_idx, test_idx) in enumerate(folds):\n",
    "\n",
    "    #train and validation set\n",
    "    train_set = lgb_train_set.subset(sorted(train_idx))\n",
    "    test_set = lgb_train_set.subset(sorted(test_idx))\n",
    "\n",
    "    print('-'*50 + '\\n')\n",
    "    print(f'Iteration {i+1}')\n",
    "\n",
    "    #train rum_boost with cross-nested arguments\n",
    "    LPMC_model_trained = rum_train(train_set, model_specification, valid_sets = [test_set])\n",
    "\n",
    "    #aggregate results\n",
    "    ce_loss += LPMC_model_trained.best_score\n",
    "    num_trees += LPMC_model_trained.best_iteration\n",
    "    \n",
    "    print('-'*50 + '\\n')\n",
    "    print(f'Best cross entropy loss: {LPMC_model_trained.best_score}')\n",
    "    print(f'Best number of trees: {LPMC_model_trained.best_iteration}')\n",
    "\n",
    "ce_loss = ce_loss/5\n",
    "num_trees = num_trees/5\n",
    "print('-'*50 + '\\n')\n",
    "print(f'Cross validation negative cross entropy loss: {ce_loss}')\n",
    "print(f'With a number of trees on average of {num_trees}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on out-of-sample data\n",
    "\n",
    "Now that we have the optimal number of trees (686), we can train the final version of the model on the full dataset, and test it on out-of-sample data with the ```predict()``` method. The dataset must be a lightgbm object in the ```predict()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params[\"num_iterations\"] = int(num_trees)\n",
    "general_params[\"early_stopping_round\"] = None\n",
    "\n",
    "LPMCCN_model_fully_trained = rum_train(lgb_train_set, model_specification)\n",
    "\n",
    "preds = LPMCCN_model_fully_trained.predict(lgb_test_set) \n",
    "ce_test = cross_entropy(preds, lgb_test_set.get_label().astype(int))\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Final negative cross-entropy on the test set: {ce_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mu$ and $\\alpha$ optimisation \n",
    "\n",
    "We optimise $\\mu$ and $\\alpha$ with scipy.minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([1.25, 1.16])  # random values\n",
    "\n",
    "alphas = np.array([[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.5, 0.5]])\n",
    "\n",
    "cross_nested_structure = {\n",
    "    \"mu\": mu,\n",
    "    \"alphas\": alphas,\n",
    "    \"optimise_mu\": True,\n",
    "    \"optimise_alphas\": np.array(\n",
    "        [[False, False], [False, False], [False, False], [True, True]]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specification = {\n",
    "    \"rum_structure\": rum_structure,\n",
    "    \"cross_nested_logit\": cross_nested_structure,\n",
    "    \"general_params\": general_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features and label column names\n",
    "features = [f for f in LPMC_train.columns if f != \"choice\"]\n",
    "label = \"choice\"\n",
    "\n",
    "#create lightgbm dataset\n",
    "lgb_train_set = lightgbm.Dataset(LPMC_train[features], label=LPMC_train[label], free_raw_data=False)\n",
    "lgb_test_set = lightgbm.Dataset(LPMC_test[features], label=LPMC_test[label], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, folds = load_preprocess_LPMC(path=\"../Data/\")\n",
    "\n",
    "ce_loss = 0\n",
    "num_trees = 0\n",
    "\n",
    "#5-fold CV\n",
    "for i, (train_idx, test_idx) in enumerate(folds):\n",
    "\n",
    "    #train and validation set\n",
    "    train_set = lgb_train_set.subset(sorted(train_idx))\n",
    "    test_set = lgb_train_set.subset(sorted(test_idx))\n",
    "\n",
    "    print('-'*50 + '\\n')\n",
    "    print(f'Iteration {i+1}')\n",
    "\n",
    "    #train rum_boost with cross-nested arguments\n",
    "    LPMC_model_trained = rum_train(train_set, model_specification, valid_sets = [test_set])\n",
    "\n",
    "    #aggregate results\n",
    "    ce_loss += LPMC_model_trained.best_score\n",
    "    num_trees += LPMC_model_trained.best_iteration\n",
    "    \n",
    "    print('-'*50 + '\\n')\n",
    "    print(f'Best cross entropy loss: {LPMC_model_trained.best_score}')\n",
    "    print(f'Best number of trees: {LPMC_model_trained.best_iteration}')\n",
    "\n",
    "ce_loss = ce_loss/5\n",
    "num_trees = num_trees/5\n",
    "print('-'*50 + '\\n')\n",
    "print(f'Cross validation negative cross entropy loss: {ce_loss}')\n",
    "print(f'With a number of trees on average of {num_trees}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on out-of-sample data\n",
    "\n",
    "Now that we have the optimal number of trees (2251), we can train the final version of the model on the full dataset, and test it on out-of-sample data with the ```predict()``` method. Note that the dataset must be a lightgbm object in the ```predict()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params[\"num_iterations\"] = int(num_trees)\n",
    "general_params[\"early_stopping_round\"] = None\n",
    "\n",
    "LPMCCN_model_fully_trained = rum_train(lgb_train_set, model_specification)\n",
    "\n",
    "preds = LPMCCN_model_fully_trained.predict(lgb_test_set) \n",
    "ce_test = cross_entropy(preds, lgb_test_set.get_label().astype(int))\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Final negative cross-entropy on the test set: {ce_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Salvadé, N., & Hillel, T. (2025). Rumboost: Gradient Boosted Random Utility Models. *Transportation Research Part C: Emerging Technologies* 170, 104897. DOI: [10.1016/j.trc.2024.104897](https://doi.org/10.1016/j.trc.2024.104897)\n",
    "\n",
    "Hillel, T., Elshafie, M.Z.E.B., Jin, Y., 2018. Recreating passenger mode choice-sets for transport simulation: A case study of London, UK. Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction 171, 29–42. https://doi.org/10.1680/jsmic.17.00018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
