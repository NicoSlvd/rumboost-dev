{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rumboost.rumboost import rum_train\n",
    "from rumboost.datasets import load_preprocess_LPMC\n",
    "from rumboost.metrics import cross_entropy\n",
    "from rumboost.datasets import prepare_dataset\n",
    "\n",
    "import numpy as np\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: GPU and batch training \n",
    "\n",
    "This notebook shows features implemented in RUMBoost through an example on the LPMC dataset, a mode choice dataset in London developed Hillel et al. (2018). You can find the original source of data [here](https://www.icevirtuallibrary.com/doi/suppl/10.1680/jsmic.17.00018) and the original paper [here](https://www.icevirtuallibrary.com/doi/full/10.1680/jsmic.17.00018).\n",
    "\n",
    "We first load the preprocessed dataset and its folds for cross-validation. You can find the data under the Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "LPMC_train, LPMC_test, folds = load_preprocess_LPMC(path=\"../Data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a RUMBoost model with gradient and hessian computed on the GPU\n",
    "\n",
    "In order to train a rumboost model with GPU you need to install `torch` and pass a dictionary to the `rum_train()` function `torch_tensors` argument with the following keys:\n",
    "`'device'`: can be cpu, gpu or cuda. We recommend cuda for best results.\n",
    "`'torch_compile'`: a boolean if torch.compile will be use to compute the gradient and hessian.\n",
    "\n",
    "Note that this is only for the calculations within RUMBoost, and therefore the calculations from lightgbm will still be in CPU. This can still provide substantial speed-up for models with heavy calculations (big datasets or for nested or cross-nested logit models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensors = {\n",
    "    'device':'cuda',\n",
    "    'torch_compile':False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "general_params = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_classes\": 4,  # important\n",
    "    \"verbosity\": 1,  # specific RUMBoost parameter\n",
    "    \"num_iterations\": 3000,\n",
    "    \"early_stopping_round\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rum_structure = [\n",
    "    {\n",
    "        \"utility\": [0],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_walking\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "    {\n",
    "        \"utility\": [1],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_cycling\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "    {\n",
    "        \"utility\": [2],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_pt_access\",\n",
    "            \"dur_pt_bus\",\n",
    "            \"dur_pt_rail\",\n",
    "            \"dur_pt_int_waiting\",\n",
    "            \"dur_pt_int_walking\",\n",
    "            \"pt_n_interchanges\",\n",
    "            \"cost_transit\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "                [17],\n",
    "                [18],\n",
    "                [19],\n",
    "                [20],\n",
    "                [21],\n",
    "                [22],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "    {\n",
    "        \"utility\": [3],\n",
    "        \"variables\": [\n",
    "            \"age\",\n",
    "            \"female\",\n",
    "            \"day_of_week\",\n",
    "            \"start_time_linear\",\n",
    "            \"car_ownership\",\n",
    "            \"driving_license\",\n",
    "            \"purpose_B\",\n",
    "            \"purpose_HBE\",\n",
    "            \"purpose_HBO\",\n",
    "            \"purpose_HBW\",\n",
    "            \"purpose_NHBO\",\n",
    "            \"fueltype_Average\",\n",
    "            \"fueltype_Diesel\",\n",
    "            \"fueltype_Hybrid\",\n",
    "            \"fueltype_Petrol\",\n",
    "            \"distance\",\n",
    "            \"dur_driving\",\n",
    "            \"cost_driving_fuel\",\n",
    "            \"congestion_charge\",\n",
    "            \"driving_traffic_percent\",\n",
    "        ],\n",
    "        \"boosting_params\": {\n",
    "            \"monotone_constraints_method\": \"advanced\",\n",
    "            \"max_depth\": 1,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"monotone_constraints\": [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "            ],\n",
    "            \"interaction_constraints\": [\n",
    "                [0],\n",
    "                [1],\n",
    "                [2],\n",
    "                [3],\n",
    "                [4],\n",
    "                [5],\n",
    "                [6],\n",
    "                [7],\n",
    "                [8],\n",
    "                [9],\n",
    "                [10],\n",
    "                [11],\n",
    "                [12],\n",
    "                [13],\n",
    "                [14],\n",
    "                [15],\n",
    "                [16],\n",
    "                [17],\n",
    "                [18],\n",
    "                [19],\n",
    "            ],\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specification = {\n",
    "    \"general_params\": general_params,\n",
    "    \"rum_structure\": rum_structure,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and label column names\n",
    "features = [f for f in LPMC_train.columns if f != \"choice\"]\n",
    "label = \"choice\"\n",
    "\n",
    "# create lightgbm dataset\n",
    "lgb_train_set = lightgbm.Dataset(\n",
    "    LPMC_train[features], label=LPMC_train[label], free_raw_data=False\n",
    ")\n",
    "lgb_test_set = lightgbm.Dataset(\n",
    "    LPMC_test[features], label=LPMC_test[label], free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params[\"num_iterations\"] = 1276\n",
    "general_params[\"early_stopping_round\"] = None\n",
    "\n",
    "LPMC_model_fully_trained = rum_train(lgb_train_set, model_specification, torch_tensors=torch_tensors)\n",
    "\n",
    "preds = LPMC_model_fully_trained.predict(lgb_test_set)\n",
    "\n",
    "ce_test = cross_entropy(preds.cpu().numpy(), lgb_test_set.get_label().astype(int))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final negative cross-entropy on the test set: {ce_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling and batch training\n",
    "\n",
    "For training speed-up it is possible to use subsampling within the RUMBoost training environment, or to directly provide batches. Batch training is efficient is it allows for not duplicating the data during training, which is useful when training with large datasets. Subsampling is more accurate than bacth training for GBDT, but requires more memory, as the full dataset needs to be passed to the RUMBoost object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params['subsampling'] = 0.5\n",
    "general_params['subsample_freq'] = 1\n",
    "\n",
    "LPMC_model_fully_trained = rum_train(lgb_train_set, model_specification, torch_tensors=torch_tensors)\n",
    "\n",
    "preds = LPMC_model_fully_trained.predict(lgb_test_set)\n",
    "\n",
    "ce_test = cross_entropy(preds.cpu().numpy(), lgb_test_set.get_label().astype(int))\n",
    "\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final negative cross-entropy on the test set: {ce_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch training\n",
    "\n",
    "We need to first already preprocess the dataset with the function `prepare_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_set, lgb_test_set = prepare_dataset(rum_structure, LPMC_train, 4, df_test=[LPMC_test], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can prepare the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices for train set\n",
    "permutations = np.random.permutation(lgb_train_set['num_data'])\n",
    "batch_size = 4000\n",
    "num_batches = len(permutations) // batch_size + 1\n",
    "batches = np.split(permutations, np.arange(batch_size, len(permutations), batch_size))\n",
    "\n",
    "#indices for test set\n",
    "permutations_valid = np.random.permutation(lgb_test_set['num_data'][0])\n",
    "batch_size_valid = 1000\n",
    "batches_valid = np.split(permutations_valid, np.arange(batch_size_valid, len(permutations_valid), batch_size_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And choose for how many iterations each batch will train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params['num_iterations'] = 30\n",
    "#reset subsampling\n",
    "general_params['subsampling'] = 1.0\n",
    "general_params['subsample_freq'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train teh model. The key point is to pass the model trained in the previous iteration as an input to the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_cross_entropy_train = 0\n",
    "epoch_cross_entropy_valid = 0\n",
    "\n",
    "time_tracker = []\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_valid_loss = []\n",
    "print('Start training...')\n",
    "\n",
    "for i in range(10*num_batches):\n",
    "\n",
    "    train_idx = batches[i%num_batches]\n",
    "    valid_idx = batches_valid[i%num_batches]\n",
    "\n",
    "    small_train_set = {}\n",
    "    small_test_set = {}\n",
    "    small_train_set['train_sets'] = []\n",
    "    small_test_set['valid_sets'] = []\n",
    "\n",
    "\n",
    "    for j, dataset in enumerate(lgb_train_set['train_sets']):\n",
    "        if rum_structure[j]['shared']:\n",
    "            dataset_train_idx = np.concatenate([np.array(train_idx) + (u - rum_structure[j]['utility'][0]) * lgb_train_set['num_data'] for u in rum_structure[j]['utility'][:len(rum_structure[j]['variables'])]])\n",
    "            dataset_valid_idx = np.concatenate([np.array(valid_idx) + (u - rum_structure[j]['utility'][0]) * lgb_test_set['num_data'][0] for u in rum_structure[j]['utility'][:len(rum_structure[j]['variables'])]])\n",
    "        else:\n",
    "            dataset_train_idx = np.array(train_idx)\n",
    "            dataset_valid_idx = np.array(valid_idx)\n",
    "        small_train_set['train_sets'].append(dataset.subset(dataset_train_idx))\n",
    "        small_test_set['valid_sets'].append([lgb_test_set['valid_sets'][0][j].subset(dataset_valid_idx)])\n",
    "\n",
    "    small_train_set['num_data'] = len(train_idx)\n",
    "    small_test_set['num_data'] = [len(valid_idx)]\n",
    "    small_train_set['labels'] = lgb_train_set['labels'][sorted(train_idx)]\n",
    "    small_test_set['valid_labels'] = [lgb_test_set['valid_labels'][0][sorted(valid_idx)]]\n",
    "    small_test_set['valid_sets'] = np.array(small_test_set['valid_sets']).T.tolist()\n",
    "\n",
    "    if i == 0:\n",
    "        init_models = None\n",
    "    else: \n",
    "        init_models = MTMC_model_fully_trained.boosters\n",
    "\n",
    "    MTMC_model_fully_trained = rum_train(small_train_set, model_specification, valid_sets=[small_test_set], torch_tensors=torch_tensors, init_models=init_models, keep_training_booster=True)\n",
    "\n",
    "    epoch_cross_entropy_train += MTMC_model_fully_trained.best_score_train\n",
    "    epoch_cross_entropy_valid += MTMC_model_fully_trained.best_score\n",
    "  \n",
    "    if (i) % num_batches == 0 and i != 0:\n",
    "        permutations = np.random.permutation(lgb_train_set['num_data'])\n",
    "        batches = np.split(permutations, np.arange(batch_size, len(permutations), batch_size))\n",
    "        permutations_valid = np.random.permutation(lgb_test_set['num_data'][0])\n",
    "        batches_valid = np.split(permutations_valid, np.arange(batch_size_valid, len(permutations_valid), batch_size_valid))\n",
    "        print(f\"Epoch: {i//num_batches}, Cross-Entropy Train: {epoch_cross_entropy_train/num_batches}, Cross-Entropy Valid: {epoch_cross_entropy_valid/num_batches}\")\n",
    "        print('new mu:', MTMC_model_fully_trained.mu)\n",
    "\n",
    "      \n",
    "        epoch_cross_entropy_train = 0\n",
    "        epoch_cross_entropy_valid = 0\n",
    "\n",
    "    epoch_train_loss.append(MTMC_model_fully_trained.best_score_train)\n",
    "    epoch_valid_loss.append(MTMC_model_fully_trained.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Salvadé, N., & Hillel, T. (2025). Rumboost: Gradient Boosted Random Utility Models. *Transportation Research Part C: Emerging Technologies* 170, 104897. DOI: [10.1016/j.trc.2024.104897](https://doi.org/10.1016/j.trc.2024.104897)\n",
    "\n",
    "Hillel, T., Elshafie, M.Z.E.B., Jin, Y., 2018. Recreating passenger mode choice-sets for transport simulation: A case study of London, UK. Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction 171, 29–42. https://doi.org/10.1680/jsmic.17.00018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
